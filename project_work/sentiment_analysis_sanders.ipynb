{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROJECT WORK (MUST TO DO )\n",
    "\n",
    "The twitter sentiment corpus created by Sanders Analytics (failed company), consists of 5513 hand-classified tweets (however, 400 tweets missing due to the scripts created by the company).\n",
    "\n",
    "Each tweet was classified with respect to one of different topics and sentiments.\n",
    "\n",
    "The dataset is available in this folder as ```tweet_sentiment.csv```.\n",
    "\n",
    "Your goal for this project will consist of:\n",
    "\n",
    "- Clean the dataset from broken data (if present);\n",
    "- (Optional) Use the NLTK library to remove the stopwords; \n",
    "- (Optional) Use the NLTK library perform stemming and lemmatization text processing;\n",
    "- Perform text vectorization with both the Count and TF-IDF vectorizers with the tokenizer provided by the NLTK library;\n",
    "- Build and train sentiment analysis classifiers with at least two ML models using scklearn;\n",
    "- Provide quantative metrics for the evaluation of your model;\n",
    "- Provide a critique qualitative analysis of both correct and wrong preditions;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funny preliminar knowledge --> recognize language with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langdetect in c:\\users\\chiqu\\appdata\\roaming\\python\\python311\\site-packages (1.0.7)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\nlp\\lib\\site-packages (from langdetect) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'en'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "detect(\"War doesn't show who's right, just who's left.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "####### \n",
    "# import all the libraries that you need\n",
    "####### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sentiment_twitter = pd.read_csv(\"tweet_sentiment.csv\")\n",
    "dataset_sentiment_twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sentiment_twitter.Topic.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sentiment_twitter.Sentiment.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Recognition\n",
    "\n",
    "As you saw, the twitter sentiment corpus created by Sanders Analytics include also the feature \"Topic\". \n",
    "\n",
    "The topics included are 'apple', 'google', 'microsoft', and 'twitter'. This categorization allows for the segmentation of tweets according to the company or service being referenced, enabling analysis of discussions related to these specific entities.\n",
    "\n",
    "Your goal for this project will consist of:\n",
    "\n",
    "- Clean the dataset from broken data (if present);\n",
    "- (Optional) Use the NLTK library to remove the stopwords; \n",
    "- (Optional) Use the NLTK library perform stemming and lemmatization text processing;\n",
    "- Perform text vectorization with both the Count and TF-IDF vectorizers with the tokenizer provided by the NLTK library;\n",
    "- Build and train topic classifiers with at least two ML models using scklearn;\n",
    "- Provide quantative metrics for the evaluation of your model;\n",
    "- Provide a critique qualitative analysis of both correct and wrong preditions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
